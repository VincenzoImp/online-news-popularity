{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 2021/2022 - Challenge \n",
    "\n",
    "<hr>\n",
    "\n",
    "# Identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yourNameSurname='Vincenzo Imperati' # e.g., yourNameSurname='Mario Rossi'\n",
    "yourMatricolaNumber='1834930' # e.g., yourMatricolaNumber='12345678'\n",
    "yourStudentEMAIL='imperati.1834930@studenti.uniroma1.it' # e.g., yourStudentEMAIL='rossim.12345678@studenti.uniroma1.it'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 1. Mandatory Rules:\n",
    "- This year the results of the challenges will count 11,2/28 of your final grade (full info about grades <a href='https://twiki.di.uniroma1.it/twiki/view/ApprAuto '>here</a>).\n",
    "- Only one submission is allowed. We will not consider multiple submissions.\n",
    "- Please remember your solution must be <b>\"YOUR SOLUTION\"</b>, hence you are requested to deliver your individual answers/arguments/opinions/critics.\n",
    "- Mail your solution (with your <b>jupyter notebook</b> and the cleaned dataset) only to stefano.faralli@uniroma1.it <b>deadlines are announced on the ML google group and <a href='https://twiki.di.uniroma1.it/twiki/view/ApprAuto'>here</a> (NO EXCEPTIONS)</b> if you miss to deliver your solution you must wait the next (if any) available deadline. \n",
    "- The subject of your email must be: \"[ML-21-22-Challenge_solution] NAME - SURNAME - MATRICOLA\".\n",
    "- Double check the subject of your email and the attachments.\n",
    "- In case you want to compress the attachment, <b>USE ONLY STANDARD ZIP compression</b> (NO RAR,7Z etc..).\n",
    "- <b>Please sumbit The notebook (with SAVED OUTPUTS) and the cleaned dataset!</b>.\n",
    "- Your solution might be considered as the \"copy\" of others solutions, in that specific case the resulting score for all involved students will be 0/8.\n",
    "- Then read carefully all the part of the jupyter notebook and fill all fields.\n",
    "- <b>solutions (and correspondig points) are evaluated mainly on your thoughts/comments/opinions</b>.  \n",
    "- If you have questions <b>Don't write \"personal\" emails</b> to Stefano Faralli, instead <b>use our google group</b>.\n",
    "- A solution having a summary discussion with less than 500 words is evaluated with 0 points.\n",
    "- Comments summary etc.. must be in <b>English</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "### Dataset and task Description:\n",
    "<img width='400' src='news-online.jpeg'/>\n",
    "\n",
    "- The challenge is about online news popularity;\n",
    "- The provided dataset consists of one single csv file (\"OnlineNewsPopularity.csv\");\n",
    "- The provided dataset is a modified <b>noisy</b> version of the original dataset described in [1];\n",
    "\n",
    "[1] K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision\n",
    "    Support System for Predicting the Popularity of Online News. Proceedings\n",
    "    of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence,\n",
    "    September, Coimbra, Portugal\n",
    "\n",
    "\n",
    "This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal of the task is to predict the number of shares in social networks (popularity).\n",
    "\n",
    "Number of Instances: <b>39,797</b> \n",
    "\n",
    "Number of Attributes: <b>61</b>\n",
    "\n",
    "Target: <b>shares</b>\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "<table>\n",
    " <tr><th> index </th><th>name</th><th>description</th></tr>\n",
    " <tr><td>0</td><td>url</td><td>URL of the article</td></tr>\n",
    " <tr><td>1</td><td>timedelta</td><td>Days between the article publication and the dataset acquisition</td></tr>\n",
    " <tr><td>2</td><td>n_tokens_title</td><td>Number of words in the title</td></tr>\n",
    " <tr><td>3</td><td>n_tokens_content</td><td>Number of words in the content</td></tr>\n",
    " <tr><td>4</td><td>n_unique_tokens</td><td>Rate of unique words in the content</td></tr>\n",
    " <tr><td>5</td><td>n_non_stop_words</td><td>Rate of non-stop words in the content</td></tr>\n",
    " <tr><td>6</td><td>n_non_stop_unique_tokens</td><td>Rate of unique non-stop words in content</td></tr>\n",
    " <tr><td>7</td><td>num_hrefs</td><td>Number of links</td></tr>\n",
    " <tr><td>8</td><td>num_self_hrefs</td><td>Number of links to other articles published by Mashable</td></tr>\n",
    " <tr><td>9</td><td>num_imgs</td><td>Number of images</td></tr>\n",
    " <tr><td>10</td><td>num_videos</td><td>Number of videos</td></tr>\n",
    " <tr><td>11</td><td>average_token_length</td><td>Average length of the words in the content</td></tr>\n",
    " <tr><td>12</td><td>num_keywords</td><td>Number of keywords in the metadata</td></tr>\n",
    " <tr><td>13</td><td>data_channel_is_lifestyle</td><td>Is data channel 'Lifestyle'?</td></tr>\n",
    " <tr><td>14</td><td>data_channel_is_entertainment</td><td>Is data channel 'Entertainment'?</td></tr>\n",
    " <tr><td>15</td><td>data_channel_is_bus</td><td>Is data channel 'Business'?</td></tr>\n",
    " <tr><td>16</td><td>data_channel_is_socmed</td><td>Is data channel 'Social Media'?</td></tr>\n",
    " <tr><td>17</td><td>data_channel_is_tech</td><td>Is data channel 'Tech'?</td></tr>\n",
    " <tr><td>18</td><td>data_channel_is_world</td><td>Is data channel 'World'?</td></tr>\n",
    " <tr><td>19</td><td>kw_min_min</td><td>Worst keyword (min. shares)</td></tr>\n",
    " <tr><td>20</td><td>kw_max_min</td><td>Worst keyword (max. shares)</td></tr>\n",
    " <tr><td>21</td><td>kw_avg_min</td><td>Worst keyword (avg. shares)</td></tr>\n",
    " <tr><td>22</td><td>kw_min_max</td><td>Best keyword (min. shares)</td></tr>\n",
    " <tr><td>23</td><td>kw_max_max</td><td>Best keyword (max. shares)</td></tr>\n",
    " <tr><td>24</td><td>kw_avg_max</td><td>Best keyword (avg. shares)</td></tr>\n",
    " <tr><td>25</td><td>kw_min_avg</td><td>Avg. keyword (min. shares)</td></tr>\n",
    " <tr><td>26</td><td>kw_max_avg</td><td>Avg. keyword (max. shares)</td></tr>\n",
    " <tr><td>27</td><td>kw_avg_avg</td><td>Avg. keyword (avg. shares)</td></tr>\n",
    " <tr><td>28</td><td>self_reference_min_shares</td><td>Min. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>29</td><td>self_reference_max_shares</td><td>Max. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>30</td><td>self_reference_avg_sharess</td><td>Avg. shares of referenced articles in Mashable</td></tr>\n",
    " <tr><td>31</td><td>weekday_is_monday</td><td>Was the article published on a Monday?</td></tr>\n",
    " <tr><td>32</td><td>weekday_is_tuesday</td><td>Was the article published on a Tuesday?</td></tr>\n",
    " <tr><td>33</td><td>weekday_is_wednesday</td><td>Was the article published on a Wednesday?</td></tr>\n",
    " <tr><td>34</td><td>weekday_is_thursday</td><td>Was the article published on a Thursday?</td></tr>\n",
    " <tr><td>35</td><td>weekday_is_friday</td><td>Was the article published on a Friday?</td></tr>\n",
    " <tr><td>36</td><td>weekday_is_saturday</td><td>Was the article published on a Saturday?</td></tr>\n",
    " <tr><td>37</td><td>weekday_is_sunday</td><td> Was the article published on a Sunday?</td></tr>\n",
    " <tr><td>38</td><td>is_weekend</td><td>Was the article published on the weekend?</td></tr>\n",
    " <tr><td>39</td><td>LDA_00</td><td>Closeness to LDA topic 0</td></tr>\n",
    " <tr><td>40</td><td>LDA_01</td><td>Closeness to LDA topic 1</td></tr>\n",
    " <tr><td>41</td><td>LDA_02</td><td>Closeness to LDA topic 2</td></tr>\n",
    " <tr><td>42</td><td>LDA_03</td><td>Closeness to LDA topic 3</td></tr>\n",
    " <tr><td>43</td><td>LDA_04</td><td>Closeness to LDA topic 4</td></tr>\n",
    " <tr><td>44</td><td>global_subjectivity</td><td>Text subjectivity</td></tr>\n",
    " <tr><td>45</td><td>global_sentiment_polarity</td><td>Text sentiment polarity</td></tr>\n",
    " <tr><td>46</td><td>global_rate_positive_words</td><td>Rate of positive words in the content</td></tr>\n",
    " <tr><td>47</td><td>global_rate_negative_words</td><td> Rate of negative words in the content</td></tr>\n",
    " <tr><td>48</td><td>rate_positive_words</td><td>Rate of positive words among non-neutral tokens</td></tr>\n",
    " <tr><td>49</td><td>rate_negative_words</td><td>Rate of negative words among non-neutral tokens</td></tr>\n",
    " <tr><td>50</td><td>avg_positive_polarity</td><td>Avg. polarity of positive words</td></tr>\n",
    " <tr><td>51</td><td>min_positive_polarity</td><td>Min. polarity of positive words</td></tr>\n",
    " <tr><td>52</td><td>max_positive_polarity</td><td>Max. polarity of positive words</td></tr>\n",
    " <tr><td>53</td><td>avg_negative_polarity</td><td>Avg. polarity of negative words</td></tr>\n",
    " <tr><td>54</td><td>min_negative_polarity</td><td>Min. polarity of negative words</td></tr>\n",
    " <tr><td>55</td><td>max_negative_polarity</td><td>Max. polarity of negative words</td></tr>\n",
    " <tr><td>56</td><td>title_subjectivity</td><td>Title subjectivity</td></tr>\n",
    " <tr><td>57</td><td>title_sentiment_polarity</td><td>Title polarity</td></tr>\n",
    " <tr><td>58</td><td>abs_title_subjectivity</td><td>Absolute subjectivity level</td></tr>\n",
    " <tr><td>59</td><td>abs_title_sentiment_polarity</td><td>Absolute polarity level</td></tr>\n",
    "     <tr><td>60</td><td>shares</td><td>Number of shares (target)</td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# 2. Pre-processing (up to 3 of 11.2 points)     \n",
    "     \n",
    "     \n",
    "## 2.1 Clean and Load the Dataset (up to 1 of 11.2 points)\n",
    "Use the following two cells (a code cell and, a markdown cell) to: \n",
    "- create a pandas DataFrame by loading a cleaned version of the \"OnlineNewsPopularity.cvs\" file.  \n",
    "- describe the identified noise and the methodology used to fix the problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Clean and Load the Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "\n",
    "def get_df(filename):\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep=',', low_memory=False)\n",
    "    except:\n",
    "        return None\n",
    "    return df\n",
    "    \n",
    "def clean_and_load_dataset(filename):\n",
    "    # try to read dataset\n",
    "    original_csv = filename\n",
    "    df = get_df(original_csv)\n",
    "    print(\"DEBUG MESSAGE: Try to load DataFrame: {}\".format(type(df)))\n",
    "\n",
    "    # write dataset in new csv file excluding rows that do not have the correct number of columns\n",
    "    old_entry_number = -1\n",
    "    column_number = 61\n",
    "    new_csv = \"new_\" + original_csv\n",
    "    with open(original_csv, \"r\") as file_object:\n",
    "        with open(new_csv, \"w\") as csv_object:\n",
    "            for row in file_object.readlines():\n",
    "                old_entry_number += 1\n",
    "                row = row[:-1].replace(' ', '').split(',')\n",
    "                if len(row) == column_number:\n",
    "                    csv.writer(csv_object).writerow(row)\n",
    "\n",
    "    # try to read dataset                \n",
    "    df = get_df(new_csv)\n",
    "    print(\"DEBUG MESSAGE: Try to load DataFrame: {}\".format(type(df)))\n",
    "\n",
    "    # drop None and 'n.a.' value\n",
    "    df.dropna()\n",
    "    df = df[(df != 'n.a.').all(axis=1)]\n",
    "\n",
    "    # cast every column to float\n",
    "    tmp_list = []\n",
    "    for col, t in zip(df.columns, df.dtypes):\n",
    "        if col != 'url' and t != float:\n",
    "            tmp_list.append(col)\n",
    "    df[tmp_list] = df[tmp_list].astype(float)\n",
    "\n",
    "    # investigation of the column domains\n",
    "    int_col = [\"timedelta\", \"n_tokens_title\", \"n_tokens_content\", \"num_hrefs\", \"num_self_hrefs\", \n",
    "               \"num_imgs\", \"num_videos\", \"num_keywords\", \"kw_min_min\", \"kw_max_min\", \"kw_min_max\", \n",
    "               \"kw_max_max\", \"self_reference_min_shares\", \"self_reference_max_shares\", \"shares\"]\n",
    "    pos_col = [\"timedelta\", \"n_tokens_title\", \"n_tokens_content\", \"num_hrefs\", \"num_self_hrefs\", \n",
    "               \"num_imgs\", \"num_videos\", \"average_token_length\", \"num_keywords\", \"self_reference_min_shares\",\n",
    "               \"self_reference_max_shares\", \"self_reference_avg_sharess\"]\n",
    "    pos_uni_col = [\"n_unique_tokens\", \"n_non_stop_words\", \"n_non_stop_unique_tokens\", \"LDA_00\", \"LDA_01\", \"LDA_02\",\n",
    "               \"LDA_03\", \"LDA_04\", \"global_subjectivity\", \"global_rate_positive_words\", \"global_rate_negative_words\",\n",
    "               \"rate_positive_words\", \"rate_negative_words\", \"avg_positive_polarity\", \"min_positive_polarity\",\n",
    "               \"max_positive_polarity\", \"title_subjectivity\", \"abs_title_subjectivity\", \"abs_title_sentiment_polarity\"]\n",
    "    abs_uni_col = [\"global_sentiment_polarity\", \"title_sentiment_polarity\"]\n",
    "    neg_uni_col = [\"avg_negative_polarity\", \"min_negative_polarity\", \"max_negative_polarity\"]\n",
    "    bin_col = [\"data_channel_is_lifestyle\", \"data_channel_is_entertainment\", \"data_channel_is_bus\", \n",
    "               \"data_channel_is_socmed\", \"data_channel_is_tech\", \"data_channel_is_world\", \"weekday_is_monday\",\n",
    "               \"weekday_is_tuesday\", \"weekday_is_wednesday\", \"weekday_is_thursday\", \"weekday_is_friday\",\n",
    "               \"weekday_is_saturday\", \"weekday_is_sunday\", \"is_weekend\"]\n",
    "\n",
    "    # drop samples that dont respect column domains\n",
    "    for col in df.columns[1:]:\n",
    "        if col in int_col:\n",
    "            tmp = df[col].apply(lambda x: int(x) != x)\n",
    "            df = df.drop(tmp[tmp == True].index, axis=0)\n",
    "        if col in pos_col:\n",
    "            tmp = df[col].apply(lambda x: x < 0)\n",
    "            df = df.drop(tmp[tmp == True].index, axis=0)\n",
    "        if col in pos_uni_col:\n",
    "            tmp = df[col].apply(lambda x: x < 0 or x > 1)\n",
    "            df = df.drop(tmp[tmp == True].index, axis=0)\n",
    "        if col in abs_uni_col:\n",
    "            tmp = df[col].apply(lambda x: x < -1 or x > 1)\n",
    "            df = df.drop(tmp[tmp == True].index, axis=0)\n",
    "        if col in neg_uni_col:\n",
    "            tmp = df[col].apply(lambda x: x < -1 or x > 0)\n",
    "            df = df.drop(tmp[tmp == True].index, axis=0)\n",
    "        if col in bin_col:\n",
    "            tmp = df[col].apply(lambda x: x != 0 and x != 1)\n",
    "            df = df.drop(tmp[tmp == True].index, axis=0)\n",
    "\n",
    "    # save clear dataset in the new csv file\n",
    "    df.to_csv(new_csv, sep=',', encoding='utf-8', index=False)\n",
    "    print(\"DEBUG MESSAGE: Cleaning and Loading Dataset done\")\n",
    "    return df, old_entry_number\n",
    "\n",
    "\n",
    "df, old_entry_number = clean_and_load_dataset(\"OnlineNewsPopularity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 2.1\n",
    "The dataset has been cleaned in the script above of all the lines that had at least one of these characteristics:\n",
    "- number of values other than the number of columns; \n",
    "- None values; \n",
    "- n.a. values;\n",
    "- values outside the domain of the respective column.\n",
    "\n",
    "Due to the small number of incorrect samples found, these have been eliminated.\n",
    "\n",
    "I attempted to load the starting dataset first, but it was not possible due to some errors in the dataset. As a result, I rewrote the dataset on a new csv file, excluding rows with an incorrect number of columns. As a result, I was able to successfully load the dataset. I then examined the domain of each column and removed the rows that did not adhere to the dimensions of all columns. Finally, I saved the dataframe `df` in a new csv file called `new_csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dataset Analysis (up to 1 of 11.2 points)\n",
    "In the following code cell (feel free to create new cells), remember to comment your code snippets:\n",
    "\n",
    "1) Print the total number of samples;\n",
    "\n",
    "2) Print a table with the first 15 samples;\n",
    "\n",
    "3) Plot the histogram distribution of \"shares\";\n",
    "\n",
    "4) A bar chart counting the attributes:  data_channel_is_lifestyle, data_channel_is_entertainment, data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1) Print the total number of samples\n",
    "print('Total number of samples at the beginning: {}'.format(old_entry_number))\n",
    "print('Total number of samples after cleaning dataset: {}'.format(df.shape[0]))\n",
    "print('Total number of samples dropped: {}'.format(old_entry_number-df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Print a table with the first 15 samples\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) Plot the histogram distribution of \"shares\"\n",
    "from matplotlib import colors\n",
    "\n",
    "def get_hist(df, feature, min_value=None, max_value=None, b=100):\n",
    "    if min_value == None:\n",
    "        min_value = df[feature].min()\n",
    "    if max_value == None:\n",
    "        max_value = df[feature].max()\n",
    "    tmp = df[df[feature] > min_value]\n",
    "    tmp = tmp[tmp[feature] < max_value]\n",
    "    plt.hist(tmp.loc[:, feature], bins=b)\n",
    "    plt.title('histogram distribution of {} (min_value: {}, max_value: {}, bins: {})'.format(feature, min_value, max_value, b))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.hist2d(pd.Series(np.array([i for i in range(tmp.loc[:, feature].shape[0])])), tmp.loc[:, feature], bins=b, norm = colors.LogNorm())\n",
    "    plt.title('2D histogram distribution of {} (min_value: {}, max_value: {}, bins: {})'.format(feature, min_value, max_value, b))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return\n",
    "\n",
    "get_hist(df, \"shares\")\n",
    "get_hist(df, \"shares\", max_value=5000)\n",
    "get_hist(df, \"shares\", max_value=1000, b=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) A bar chart counting the attributes: data_channel_is_lifestyle, data_channel_is_entertainment,\n",
    "#   data_channel_is_bus, data_channel_is_socmed, data_channel_is_tech, data_channel_is_world\n",
    "def request_2_2_4():\n",
    "    l = ['data_channel_is_lifestyle', 'data_channel_is_entertainment', 'data_channel_is_bus', \n",
    "         'data_channel_is_socmed', 'data_channel_is_tech', 'data_channel_is_world']\n",
    "    b = pd.DataFrame({'features':[], 'samples':[]})\n",
    "    for f in l:\n",
    "        b = b.append({'features': f, 'samples': df[df[f]==1].shape[0]}, ignore_index=True)\n",
    "    ax = b.plot.bar(x='features', y='samples', rot=90)\n",
    "    return\n",
    "\n",
    "request_2_2_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 2.2\n",
    "The four requests have been resolved in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature importance analysis  (up to 1 of 11.2 points)\n",
    "\n",
    "Perform feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x, y\n",
    "df = df.drop('url', axis=1, errors='ignore')\n",
    "x = df.drop('shares', axis=1)\n",
    "y = df.shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# compute feature_importances\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(x, y)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature_importances\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.barh(x.columns[sorted_idx], rf.feature_importances_[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove features with feature_importances < 0.01\n",
    "for importance, feature in zip(rf.feature_importances_, x.columns):\n",
    "    if importance < 0.02:\n",
    "        x = x.drop(feature, axis=1)\n",
    "\n",
    "# compute feature_importances again\n",
    "rf.fit(x, y)\n",
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot feature_importances again\n",
    "sorted_idx = rf.feature_importances_.argsort()\n",
    "plt.barh(x.columns[sorted_idx], rf.feature_importances_[sorted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del rf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 2.3\n",
    "On the entire dataset, a random forest was used to recognize the most relevant features. This enabled the classification of the importance of the characteristics. Due to a large number of features available, it was decided to consider only the most important. As a result, the features less important than 0.02 have been removed. At the end of this operation, there are 18 features left, accounting for roughly one-third of the total. The importance of the remaining features has been recalculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Model Selection (up to 8.2 of 11.2  points)\n",
    "In this part of the challenge you are requested to perform all the necessary steps required in order to design a full fledged classification task on the <b>shares</b> feature.\n",
    "\n",
    "You are requested to perform the following steps having in mind the following: \n",
    "\n",
    "1) the dataset must be properly splitted to perform crossvalidation \n",
    "\n",
    "2) when required, features must be properly encoded\n",
    "\n",
    "3) in order to simplify the problem the target feature can be dicretized <b>(number of classes must be >=5)</b> ;\n",
    "\n",
    "4) for model selection you are requested to consider: \n",
    "\n",
    "- Decision Trees\n",
    "\n",
    "- Support Vector Machines;\n",
    "\n",
    "- An ensamble methodology;\n",
    "\n",
    "- MLPNs.\n",
    "\n",
    "5) hyper-parameter tuning <b>must</b> be performed and discussed;\n",
    "\n",
    "6) apply standardizion and normalization when appropriate;\n",
    "\n",
    "7) remember to use an appropriate evaluation setting (cross-fold etc..)\n",
    "\n",
    "8) describe the measures adopted for the evaluation and discuss the results;\n",
    "\n",
    "9) provide a discussion of the model selection, where you describe the differences in terms of performance and explains the root causes;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create `new_df` which contains only the features chosen to train the models and 'shares'. \n",
    "The value `entry_number_step1` represents the number of samples considered at this point in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()\n",
    "columns = []\n",
    "for col in df.columns:\n",
    "    if col not in x.columns and col != 'shares':\n",
    "        new_df = new_df.drop(col, axis=1, errors='ignore')\n",
    "    else:\n",
    "        columns.append(col)\n",
    "entry_number_step1 = new_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i plot hist and hist2d for every feature to see the distribution of the data. Some features have distinct outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in new_df.columns:\n",
    "    get_hist(new_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the following script, I eliminate the outliers. I also tried the other way (with the commented scripts) but in the end, I found it better to eliminate the outliers via zscore. It is reasonable to think that the data at our disposal may have some noise. It is better to remove it for better evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "z_scores = zscore(new_df)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 5).all(axis=1)\n",
    "new_df = new_df[filtered_entries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I rerun the plot of hist and hist2d for each feature noting some differences. Without the outliers, the distributions are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in new_df.columns:\n",
    "    get_hist(new_df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of samples eliminated is 1875, and the number of samples remaining is 37638. I believe it is correct to remove this number of samples, thereby removing data noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entry_number_step2 = new_df.shape[0]\n",
    "print('Samples before dropping outliers: {}'.format(entry_number_step1))\n",
    "print('Samples after dropping outliers: {}'.format(entry_number_step2))\n",
    "print('Samples removed:',format(entry_number_step1-entry_number_step2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features (`x`) are scaled, and 'shares' (`y`) is discretized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "x = new_df.drop('shares', axis=1)\n",
    "y = new_df.shares\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_scal = scaler.fit_transform(x)\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')\n",
    "y_disc = discretizer.fit_transform(y.array.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x\n",
    "del y\n",
    "del new_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then proceed to divide the dataset into `x_train`, `y_train`. `x_test`, `y_test`. I will only use the first two to train the models. and I will use the last two only and exclusively to test the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scal, y_disc, shuffle=True, stratify=y_disc, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the dictionary of models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "models = {\n",
    "    'DT': DecisionTreeClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'BOOST': AdaBoostClassifier(random_state=42),\n",
    "    'RND_FOREST': RandomForestClassifier(random_state=42),\n",
    "    'MLPN': MLPClassifier(hidden_layer_sizes=(25, 25), random_state=42, max_iter=10000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create the three useful functions to train and evaluate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def fit_models(models, x_train, y_train):\n",
    "    for name in models.keys():\n",
    "        print('fitting {}'.format(name))\n",
    "        models[name].fit(x_train, y_train.ravel())\n",
    "        print('done')\n",
    "    return\n",
    "    \n",
    "def evaluate_models(models, x_test, y_test, x_scal, y_disc):\n",
    "    for name, model in models.items():\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print('name: {}'.format(name))\n",
    "        print()\n",
    "        print('params:')\n",
    "        print(model.get_params())\n",
    "        print()\n",
    "        y_pred = model.predict(x_test)\n",
    "        report = classification_report(y_test, y_pred, zero_division=0)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cv_accuracy = cross_val_score(model, x_scal, y_disc.ravel(), n_jobs=-1, scoring='accuracy')\n",
    "        cv_f1_macro = cross_val_score(model, x_scal, y_disc.ravel(), n_jobs=-1, scoring='f1_macro')\n",
    "        print('report:')\n",
    "        print(report)\n",
    "        print()\n",
    "        print('confusion_matrix:')\n",
    "        print(cm)\n",
    "        print()\n",
    "        print('cross_val_score (accuracy):')\n",
    "        print(cv_accuracy)\n",
    "        print(\"%0.4f (+/- %0.4f)\" % (cv_accuracy.mean(), cv_accuracy.std() * 2))\n",
    "        print()\n",
    "        del cv_accuracy\n",
    "        gc.collect()\n",
    "        print('cross_val_score (f1_macro):')\n",
    "        print(cv_f1_macro)\n",
    "        print(\"%0.4f (+/- %0.4f)\" % (cv_f1_macro.mean(), cv_f1_macro.std() * 2))\n",
    "        del cv_f1_macro\n",
    "        gc.collect()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the models, I print the dimensions of the train and test dataset. it is useful to stick to a fixed proportion to correctly perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = round((x_train.shape[0]/(x_train.shape[0]+x_test.shape[0]))*100, 2)\n",
    "print('train set dimention: {} ({}%)'.format(x_train.shape[0], train_perc))\n",
    "print('test set dimention: {} ({}%)'.format(x_test.shape[0], 100-train_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models(models, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_models(models, x_test, y_test, x_scal, y_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary evaluation:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> name </th><th>accuracy</th><th>f1macro</th><th>cv_accuracy</th><th>cv_f1macro</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DT</td><td>0.66</td><td>0.21</td><td>0.4071 (+/- 0.3962)</td><td>0.1513 (+/- 0.1054)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SVM</td><td>0.80</td><td>0.18</td><td>0.7968 (+/- 0.0001)</td><td>0.1774 (+/- 0.0000)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BOOST</td><td>0.80</td><td>0.18</td><td>0.7957 (+/- 0.0037)</td><td>0.1783 (+/- 0.0023)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RND_FOREST</td><td>0.80</td><td>0.18</td><td>0.6614 (+/- 0.5165)</td><td>0.1570 (+/- 0.1051)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MLPN</td><td>0.80</td><td>0.18</td><td>0.7963 (+/- 0.0027)</td><td>0.1828 (+/- 0.0091)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "It was discovered that the unbalanced dataset resulted in excellent accuracy but a very poor f1_macro. This is due to the imbalanced dataset. We want to correctly predict all classes, we cannot predict exactly class 0 only. for this we want to maximize f1_macro while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the models have been trained with a large amount of class 0 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the intent is to balance the classes. target:\n",
    "- class 0: 2/7 samples\n",
    "- class 1: 2/7 samples\n",
    "- class 2: 1/7 samples\n",
    "- class 3: 1/7 samples\n",
    "- class 4: 1/7 samples\n",
    "\n",
    "I tried to train the models with a dataset having the same number of samples for each class, but the accuracy drops drastically compared to this proposed solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = min(np.count_nonzero(y_train == 0), int((2/7)*y_train.shape[0]))\n",
    "c1 = min(np.count_nonzero(y_train == 1), int((2/7)*y_train.shape[0]))\n",
    "c2 = min(np.count_nonzero(y_train == 2), int((1/7)*y_train.shape[0]))\n",
    "c3 = min(np.count_nonzero(y_train == 3), int((1/7)*y_train.shape[0]))\n",
    "c4 = min(np.count_nonzero(y_train == 4), int((1/7)*y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use RandomUnderSampler to undersampling classes that contain more samples than desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy={0:c0, 1:c1, 2:c2, 3:c3, 4:c4}, random_state=42)\n",
    "x_train_under, y_train_under = rus.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rus\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_under_dim = x_train_under.shape[0]\n",
    "print('Samples before undersampling: {}'.format(x_train.shape[0]))\n",
    "print('Samples after undersampling: {}'.format(x_train_under_dim))\n",
    "print('Samples removed: {}'.format(x_train.shape[0]-x_train_under_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train_under, bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use SMOTE to oversampling classes that contain less samples than desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = max(np.count_nonzero(y_train_under == 0), int((2/7)*y_train.shape[0]))\n",
    "c1 = max(np.count_nonzero(y_train_under == 1), int((2/7)*y_train.shape[0]))\n",
    "c2 = max(np.count_nonzero(y_train_under == 2), int((1/7)*y_train.shape[0]))\n",
    "c3 = max(np.count_nonzero(y_train_under == 3), int((1/7)*y_train.shape[0]))\n",
    "c4 = max(np.count_nonzero(y_train_under == 4), int((1/7)*y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(k_neighbors=3, n_jobs=-1, sampling_strategy={0:c0,1:c1,2:c2,3:c3,4:c4}, random_state=42)\n",
    "x_train_over, y_train_over = sm.fit_resample(x_train_under, y_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sm\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_number_step4 = x_train_over.shape[0]\n",
    "print('Samples before oversampling: {}'.format(x_train_under_dim))\n",
    "print('Samples after oversampling: {}'.format(entry_number_step4))\n",
    "print('Samples added: {}'.format(entry_number_step4-x_train_under_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train_under\n",
    "del y_train_under\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the samples are balanced as originally planned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train_over, bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plot the distribution of the features following the new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(columns):\n",
    "    if col != 'shares':\n",
    "        to_plot = x_train_over[:,i]\n",
    "    else:\n",
    "        to_plot = y_train_over\n",
    "    min_value = to_plot.min()\n",
    "    max_value = to_plot.max()\n",
    "    b = 100\n",
    "    plt.hist(to_plot, bins=b)\n",
    "    plt.title('histogram distribution of {} (min_value: {}, max_value: {}, bins: {})'.format(col, min_value, max_value, b))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.hist2d(pd.Series(np.array([i for i in range(to_plot.shape[0])])), to_plot, bins=b, norm = colors.LogNorm())\n",
    "    plt.title('2D histogram distribution of {} (min_value: {}, max_value: {}, bins: {})'.format(col, min_value, max_value, b))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = round((x_train_over.shape[0]/(x_train_over.shape[0]+x_test.shape[0]))*100, 2)\n",
    "print('train set dimention: {} ({}%)'.format(x_train_over.shape[0], train_perc))\n",
    "print('test set dimention: {} ({}%)'.format(x_test.shape[0], 100-train_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fit and evaluate models with 2/3 of training set and 1/3 of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit_models(models, x_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_models(models, x_test, y_test, x_scal, y_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary evaluation:\n",
    "<table>\n",
    "    <tr>\n",
    "        <th> name </th><th>accuracy</th><th>f1macro</th><th>cv_accuracy</th><th>cv_f1macro</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>DT</td><td>0.48</td><td>0.20</td><td>0.4071 (+/- 0.3962)</td><td>0.1513 (+/- 0.1054)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SVM</td><td>0.58</td><td>0.23</td><td>0.7968 (+/- 0.0001)</td><td>0.1774 (+/- 0.0000)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BOOST</td><td>0.54</td><td>0.21</td><td>0.7957 (+/- 0.0037)</td><td>0.1783 (+/- 0.0023)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RND_FOREST</td><td>0.66</td><td>0.25</td><td>0.6614 (+/- 0.5165)</td><td>0.1570 (+/- 0.1051)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MLPN</td><td>0.57</td><td>0.23</td><td>0.7963 (+/- 0.0027)</td><td>0.1828 (+/- 0.0091)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "With under/over sampling a better f1_macro is noted, while maintaining good accuracy. To choose the best model I consider cv_f1macro and cv_accuracy. I selected MLPN as the best model ad I proceed with the hyper-parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define function for model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_model(model, param_grid, scoring, x_train, y_train, grid_jobs):\n",
    "    print('tuning...')\n",
    "    clf = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, verbose=1, n_jobs=grid_jobs)\n",
    "    clf.fit(x_train, y_train.ravel())\n",
    "    print('done')\n",
    "    print()\n",
    "    print(\"Best: %f using %s\" % (clf.best_score_, clf.best_params_))\n",
    "    best_params = clf.best_params_.copy()\n",
    "    del clf\n",
    "    gc.collect()\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I perform tuning on half of the test dataset, or one-third of the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train_tuning, bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train\n",
    "del y_train\n",
    "del models\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set param grid for tuning model with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPN_param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 50), (25, 25), (25, 25, 25)],\n",
    "    'alpha': [0.0001, 0.005],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'random_state': [42],\n",
    "    'max_iter': [10000]\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tune model by accuracy and i find `best_accuracy_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy_params = tune_model(MLPClassifier(), MLPN_param_grid, 'accuracy', x_train_over, y_train_over, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tune model by f1_macro and i find `best_f1macro_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1macro_params = tune_model(MLPClassifier(), MLPN_param_grid, 'f1_macro', x_train_over, y_train_over, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_models = {\n",
    "    'MLPN_accuracy': MLPClassifier(**best_accuracy_params),\n",
    "    'MLPN_f1macro': MLPClassifier(**best_f1macro_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_models(tuned_models, x_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(tuned_models, x_test, y_test, x_scal, y_disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note 3\n",
    "The outliers were removed from the entire dataset using zscore, assuming there was noise in the dataset. The dataset has been divided into two sections: train and test. Four distinct models were trained, tested, and evaluated. It was discovered that the unbalanced dataset resulted in excellent accuracy but a very poor f1_macro. \n",
    "The larger classes were undersampled, while the smaller classes were oversampled. The first two classes each have 2/7 samples, while the last three classes each have 1/7 samples.\n",
    "This division still keeps the classes unbalanced, but it is much less severe than it was at the start. Furthermore, I think that this sample division is more useful for solving the problem at hand than having the same number of samples for all classes. This is because, given the nature of the problem (prediction of the share of articles), I think that classes 0 and 1 will be far more populated than classes 2, 3, and 4. Non-viral articles outnumber viral articles.\n",
    "With the new dataset, models were trained, tested, and evaluated. These demonstrate a significant improvement in f1_macro. SVM was chosen as the best model and it was tuned. Because of the computational complexity of SVM, the tuning was done on a smaller grid than I would have preferred.\n",
    "The SVM with the following parameters {} is the best model for solving the problem The resulting performance demonstrates this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#Â 4. Summary\n",
    "Provide a summary discussion (in English) of your solution <b>(at least 500 words)</b> feel free to include plots figures and tables.\n",
    "\n",
    "<b>This is a mandatory step</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write here <b>your own</b> summary dicussion (in English) use at least 500 words, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
